{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processing Beijing_original.csv...\n",
      "--------------------------------------------------\n",
      "Column: carbon_emissions (ton)\n",
      "Zero Count: 6\n",
      "Zero Percentage: 0.13080444735120994%\n",
      "\n",
      "Column: population (unit)\n",
      "Zero Count: 0\n",
      "Zero Percentage: 0.0%\n",
      "\n",
      "Column: gdp (million yuan)\n",
      "Zero Count: 15\n",
      "Zero Percentage: 0.3270111183780248%\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing Shanghai_original.csv...\n",
      "--------------------------------------------------\n",
      "Column: carbon_emissions (ton)\n",
      "Zero Count: 676\n",
      "Zero Percentage: 13.080495356037153%\n",
      "\n",
      "Column: population (unit)\n",
      "Zero Count: 514\n",
      "Zero Percentage: 9.945820433436532%\n",
      "\n",
      "Column: gdp (million yuan)\n",
      "Zero Count: 4\n",
      "Zero Percentage: 0.07739938080495357%\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing Guangzhou_original.csv...\n",
      "--------------------------------------------------\n",
      "Column: carbon_emissions (ton)\n",
      "Zero Count: 133\n",
      "Zero Percentage: 3.926778860348391%\n",
      "\n",
      "Column: population (unit)\n",
      "Zero Count: 15\n",
      "Zero Percentage: 0.44286979627989376%\n",
      "\n",
      "Column: gdp (million yuan)\n",
      "Zero Count: 0\n",
      "Zero Percentage: 0.0%\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing Shenzhen_original.csv...\n",
      "--------------------------------------------------\n",
      "Column: carbon_emissions (ton)\n",
      "Zero Count: 121\n",
      "Zero Percentage: 3.57565011820331%\n",
      "\n",
      "Column: population (unit)\n",
      "Zero Count: 396\n",
      "Zero Percentage: 11.702127659574469%\n",
      "\n",
      "Column: gdp (million yuan)\n",
      "Zero Count: 506\n",
      "Zero Percentage: 14.952718676122931%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_zeros_in_columns(csv_file, columns_to_process):\n",
    "    \"\"\"\n",
    "    Counts the number of zeros in specified columns of a CSV file and calculates the percentage of zeros.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file (str): The path to the CSV file.\n",
    "    columns_to_process (list): A list of column names to process.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the statistics for each column. The keys are the column names and the values are dictionaries with 'Zero Count' and 'Zero Percentage (%)' as keys.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Initialize a dictionary to store the statistics for each column\n",
    "    column_stats = {}\n",
    "\n",
    "    # Iterate over the specified column names\n",
    "    for column_name in columns_to_process:\n",
    "        # Make sure the column name exists in the data\n",
    "        if column_name in data.columns:\n",
    "            # Count the number of data points with a value of 0 in the column\n",
    "            zero_count = (data[column_name] == 0).sum()\n",
    "            # Calculate the percentage of data points with a value of 0\n",
    "            zero_percentage = (zero_count / len(data)) * 100\n",
    "\n",
    "            # Store the statistics\n",
    "            column_stats[column_name] = {\n",
    "                'Zero Count': zero_count,\n",
    "                'Zero Percentage (%)': zero_percentage\n",
    "            }\n",
    "        else:\n",
    "            # If the column name does not exist in the data, provide a corresponding message\n",
    "            column_stats[column_name] = {\n",
    "                'Zero Count': 'Column not found',\n",
    "                'Zero Percentage (%)': 'Column not found'\n",
    "            }\n",
    "\n",
    "    return column_stats\n",
    "\n",
    "\n",
    "# Test the function\n",
    "csv_file_paths = ['Beijing_original.csv', 'Shanghai_original.csv', 'Guangzhou_original.csv', 'Shenzhen_original.csv']\n",
    "columns_to_process = ['carbon_emissions (ton)', 'population (unit)', 'gdp (million yuan)']  \n",
    "for csv_file_path in csv_file_paths:\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Processing {csv_file_path}...\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    result = count_zeros_in_columns(csv_file_path, columns_to_process)\n",
    "    for column_name, stats in result.items():\n",
    "        print(f\"Column: {column_name}\")\n",
    "        print(f\"Zero Count: {stats['Zero Count']}\")\n",
    "        print(f\"Zero Percentage: {stats['Zero Percentage (%)']}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_with_zeros(csv_file, columns_to_check):\n",
    "    \"\"\"\n",
    "    Remove rows from a CSV file that contain zeros in specified columns.\n",
    "\n",
    "    Args:\n",
    "        csv_file (pandas.DataFrame): The CSV file as a pandas DataFrame.\n",
    "        columns_to_check (list): A list of column names to check for zeros.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The updated CSV file with rows containing zeros removed.\n",
    "    \"\"\"\n",
    "    data = csv_file\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if any(row[column] == 0 for column in columns_to_check):\n",
    "            data = data.drop(index)\n",
    "    return data\n",
    "\n",
    "\n",
    "def apply_log_transformation(csv_file, columns_to_transform):\n",
    "    \"\"\"\n",
    "    Applies a logarithmic transformation to specified columns in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file (pandas.DataFrame): The CSV file to transform.\n",
    "    columns_to_transform (list): A list of column names to apply the transformation to.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The transformed CSV file.\n",
    "    \"\"\"\n",
    "    data = csv_file\n",
    "\n",
    "    for column_name in columns_to_transform:\n",
    "        if column_name in data.columns:\n",
    "            data[column_name] = np.log(data[column_name])\n",
    "        else:\n",
    "            print(f\"Column '{column_name}' not found in the CSV file.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satellite_img_name</th>\n",
       "      <th>BD09 coordinate</th>\n",
       "      <th>WGS84 coordinate</th>\n",
       "      <th>carbon_emissions (ton)</th>\n",
       "      <th>population (unit)</th>\n",
       "      <th>gdp (million yuan)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shanghai/16_13167_3525_s.jpg</td>\n",
       "      <td>(13167,3525)</td>\n",
       "      <td>(30.99272409355233, 121.11860375494845)</td>\n",
       "      <td>248.64958</td>\n",
       "      <td>1139</td>\n",
       "      <td>15321.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai/16_13232_3555_s.jpg</td>\n",
       "      <td>(13232,3555)</td>\n",
       "      <td>(31.23017054642347, 121.71651590238979)</td>\n",
       "      <td>730.37244</td>\n",
       "      <td>5294</td>\n",
       "      <td>47379.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shanghai/16_13223_3531_s.jpg</td>\n",
       "      <td>(13223,3531)</td>\n",
       "      <td>(31.04026134561536, 121.63372806659022)</td>\n",
       "      <td>377.98390</td>\n",
       "      <td>3943</td>\n",
       "      <td>8707.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shanghai/16_13239_3552_s.jpg</td>\n",
       "      <td>(13239,3552)</td>\n",
       "      <td>(31.206452906350798, 121.78090644134501)</td>\n",
       "      <td>675.47314</td>\n",
       "      <td>1154</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shanghai/16_13208_3537_s.jpg</td>\n",
       "      <td>(13208,3537)</td>\n",
       "      <td>(31.087774637460512, 121.4957483402576)</td>\n",
       "      <td>965.55444</td>\n",
       "      <td>3901</td>\n",
       "      <td>48677.527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             satellite_img_name BD09 coordinate  \\\n",
       "0  Shanghai/16_13167_3525_s.jpg    (13167,3525)   \n",
       "1  Shanghai/16_13232_3555_s.jpg    (13232,3555)   \n",
       "2  Shanghai/16_13223_3531_s.jpg    (13223,3531)   \n",
       "3  Shanghai/16_13239_3552_s.jpg    (13239,3552)   \n",
       "4  Shanghai/16_13208_3537_s.jpg    (13208,3537)   \n",
       "\n",
       "                           WGS84 coordinate  carbon_emissions (ton)  \\\n",
       "0   (30.99272409355233, 121.11860375494845)               248.64958   \n",
       "1   (31.23017054642347, 121.71651590238979)               730.37244   \n",
       "2   (31.04026134561536, 121.63372806659022)               377.98390   \n",
       "3  (31.206452906350798, 121.78090644134501)               675.47314   \n",
       "4   (31.087774637460512, 121.4957483402576)               965.55444   \n",
       "\n",
       "   population (unit)  gdp (million yuan)  \n",
       "0               1139           15321.365  \n",
       "1               5294           47379.030  \n",
       "2               3943            8707.273  \n",
       "3               1154               0.000  \n",
       "4               3901           48677.527  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'Shanghai_original.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_mapping = {\n",
    "    'BD09 coordinate': 'Coordinate',\n",
    "    'carbon_emissions (ton)': 'carbon',\n",
    "    'population (unit)': 'population',\n",
    "    'gdp (million yuan)': 'gdp'\n",
    "}\n",
    "\n",
    "columns_to_delete = ['satellite_img_name', 'WGS84 coordinate']\n",
    "columns_to_check = ['carbon', 'population', 'gdp']\n",
    "columns_to_transform = ['carbon', 'population', 'gdp']\n",
    "\n",
    "data = data.rename(columns=column_name_mapping)\n",
    "data = data.drop(columns=columns_to_delete)\n",
    "data = remove_rows_with_zeros(data, columns_to_check)\n",
    "data = apply_log_transformation(data, columns_to_transform)\n",
    "\n",
    "data.to_csv('Shanghai.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Beijing.csv processed and saved to Beijing_refined.csv\n",
      "File Shanghai.csv processed and saved to Shanghai_refined.csv\n",
      "File Guangzhou.csv processed and saved to Guangzhou_refined.csv\n",
      "File Shenzhen.csv processed and saved to Shenzhen_refined.csv\n"
     ]
    }
   ],
   "source": [
    "def process_and_save(file_paths):\n",
    "    \"\"\"\n",
    "    Process the given CSV files by normalizing the specified columns and save the processed data to new files.\n",
    "\n",
    "    Args:\n",
    "        file_paths (list): A list of file paths to the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        columns_to_process = ['carbon', 'population', 'gdp']\n",
    "        means = df[columns_to_process].mean()\n",
    "        mins = df[columns_to_process].min()\n",
    "        maxs = df[columns_to_process].max()\n",
    "\n",
    "        for column in columns_to_process:\n",
    "            range_width = maxs[column] - mins[column]\n",
    "            # Normalize the data, scales the values to be centered around the mean and within a range of -0.5 to 0.5.\n",
    "            df[column] = df[column].apply(lambda x: ((x - mins[column]) / range_width - 0.5) * 1.0 + means[column])\n",
    "\n",
    "        output_file_path = file_path.replace('.csv', '_refined.csv')\n",
    "\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"File {file_path} processed and saved to {output_file_path}\")\n",
    "\n",
    "csv_files = [\n",
    "    'Beijing.csv',\n",
    "    'Shanghai.csv',\n",
    "    'Guangzhou.csv',\n",
    "    'Shenzhen.csv'\n",
    "]\n",
    "\n",
    "process_and_save(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training dataset file: Beijing_train.csv\n",
      "Saved testing dataset file: Beijing_test.csv\n",
      "Saved training dataset file: Shanghai_train.csv\n",
      "Saved testing dataset file: Shanghai_test.csv\n",
      "Saved training dataset file: Guangzhou_train.csv\n",
      "Saved testing dataset file: Guangzhou_test.csv\n",
      "Saved training dataset file: Shenzhen_train.csv\n",
      "Saved testing dataset file: Shenzhen_test.csv\n"
     ]
    }
   ],
   "source": [
    "def split_and_save_datasets(csv_filenames, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits the given CSV files into training and testing datasets based on the specified train_ratio.\n",
    "    Saves the training and testing datasets as new CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_filenames (list): List of CSV file names to be processed.\n",
    "    - train_ratio (float): The ratio of data to be used for training. Default is 0.8.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for filename in csv_filenames:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Shuffle the data randomly\n",
    "        df = df.sample(frac=1, random_state=42)\n",
    "        \n",
    "        # Calculate the split point for training and testing\n",
    "        split_point = int(len(df) * train_ratio)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        train_data = df[:split_point]\n",
    "        test_data = df[split_point:]\n",
    "        \n",
    "        # Save the training and testing sets as new CSV files\n",
    "        train_filename = os.path.splitext(filename)[0] + \"_train.csv\"\n",
    "        test_filename = os.path.splitext(filename)[0] + \"_test.csv\"\n",
    "        \n",
    "        train_data.to_csv(train_filename, index=False)\n",
    "        test_data.to_csv(test_filename, index=False)\n",
    "        \n",
    "        print(f\"Saved training dataset file: {train_filename}\")\n",
    "        print(f\"Saved testing dataset file: {test_filename}\")\n",
    "\n",
    "csv_filenames = [\"Beijing_refined.csv\", \"Shanghai_refined.csv\", \"Guangzhou_refined.csv\", \"Shenzhen_refined.csv\"]\n",
    "split_and_save_datasets(csv_filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('urbanclip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdb43bb49353cb1dca8269ea39caa5689a0e733671215c0cee3fb0b5739eea66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
